---
title: "Scripts"
author: "Pia Chouaifaty"
date: "11/4/2020"
output: html_document
---

### Reading Files

*CSV*
```{r csv}
college = read.csv("/Users/piachouaifaty/ISLR_Data/College.csv")
#read.csv2 in case the separator is a semicolon instead of a comma
#read.delim 
#na.strings = "EMPTY" to set NA values as "EMPTY" instead
```

*TXT*
```{r txt}
Auto=read.table("/Users/piachouaifaty/ISLR_Data/Auto.data", header=T, na.strings = "?")
```

*EXCEL*
```{r excel}
library(xlsx)
tbl1 = read.xlsx("/Users/piachouaifaty/Book1.xlsx", sheetIndex = 1)
```

### Data Pre-Processing

*Checking Dimensions*
```{r dimensions} 
dim(dataset)
```

*Setting Rownames*
```{r setting rownames}
rownames(college)=college[,1]
```

*Deleting column*
```{r deleting columns}
college=college[,-1]
```

*Standardizing the data (usually for KNN)*
```{r standardizing for KNN}
standardizedX=scale(Dataset[,-qualitative_variable_index])
#sd=1, mean=0 for every predictor
```



#### Dealing with NAs

*Checking how many NAs*
```{r how many NAs}
length(which(is.na(dataset)))
```

*Removing NAs*
```{r removing NAs}
#(only if explicitly asked)
Dataset=na.omit(Dataset)
```

*Replacing NAs with Average*

```{r column averaging}
dataset$column_header = ifelse(is.na(dataset$column_header), ave(dataset$column_header, FUN = function(x) mean(x, na.rm = 'TRUE')), dataset$column_header)

#The above code blocks check for missing values in the age and salary columns and update the missing cells with the column-wise average.
#dataset$column_header: Selects the column in the dataset specified after $ (age and salary).
#is.na(dataset$column_header): This method returns true for all the cells in the specified column with no values.
#ave(dataset$column_header, FUN = function(x) mean(x, na.rm = ‘TRUE’)): Ths method calculates the average of the column passed as argument.

dataset$col_to_round = as.numeric(format(round(dataset$col_to_round, 0)))
#if we have a value that shouldn't be in decimal like age for ex, optional
```

#### Statistics about Predictors

*Ranges*
```{r ranges for quantitative}
v=c("mpg", "cylinders", "displacement", "horsepower", "weight", "acceleration", "year")
# v is a vector of quantitative predictors
sapply(Dataset[,v], range)

#if they are already all quantitative
t(sapply(dataset, range))
```

*Means, SDs, Medians*

```{r means, sds, medians}
#where v is a vector of the quantitative predictor names

sapply(Dataset[,v], mean)
sapply(Dataset[,v], sd)
sapply(Dataset[,v], median)

#for individual predictor
mean(Dataset$predictor)
sd(Dataset$predictor)
median(Dataset$predictor)
```

*Standard Error of Sample Mean of Predictor*
```{r standard error of sample mean}
pred.err = sd(pred)/sqrt(length(pred))
#Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations
```

*Percentiles & Quantiles*
```{r Percentiles & Quantiles}
perc = quantile(predictor, c(0.1)) #10th percentile of predictor
```

*Min & Max*
```{r min & max}
#values for all predictors for observation with the max value in "predictor"
t(subset(Dataset, predictor == min(Dataset$predictor)))
t(subset(Dataset, predictor == max(Dataset$predictor)))
```

*Removing Observations*
```{r removing observations}
#(Range)
Dataset2 = Dataset[-(10:85),] 
```

*As factor*
```{r as factor for binary qualitative predictior}
#CAREFUL, if you declare as factor, CANNOT CALCULATE COR
dataset$predictor=as.factor(dataset$predictor)
```

*Creating new Predictor, Binning*
```{r binning - new predictor}
#1. New predictor "Elite"
Elite=rep("No", nrow(college))

#2. Condition for new predictor
Elite[college$Top10perc>50]="Yes"

#3. Transform to qualitative binary factor
Elite=as.factor(Elite)

#4. Add to dataset
college=cbind(college, Elite)
#Auto = data.frame(Auto, mpg01)

```

*Checking how many observations satisfy a condition*
```{r observations that satisfy a condition}

length((Dataset$predictor[Dataset$predictor>20]))
#or
dim(subset(Dataset, predictor == 1)) 
```

*95 % confidence interval for the mean of a predictor*
```{r confidence interval}
t.test(predictor)
```


### Plotting

```{r output dimensions}
par(mfrow=c(1,1))
```

*Scatterplot*
```{r Scatterplot}
#PAIRED SCATTERPLOT
pairs(Smarket, col=Smarket$Direction) #[can only use quantitative & qualitative AFTER transforming to factor]
#col to color by category

#SCATTERPLOT OF 2 PREDICTORS
plot(college$Accept/college$Apps, college$S.F.Ratio) 
#or
plot(predyaxis~predxaxis, Dataset)
#labels: ylab="", xlab=""
```

*Plotting Line in Scatterplot*
```{r abline & points}
#for a model
abline(fit, lwd=3, col="red")
#for a known line
abline(a, b) #{intercept a, slope b}

#for non-linear models:
points(Y, fitted(fit), col="red", pch=20)

#legend for multiple lines on plot
abline()
abline()
legend(-1, legend = c("model 1", "model 2"), col=2:3, lwd=3)

```

*Boxplot*
```{r Boxplot}
plot(dataset$quantitativepredictor, dataset$anypredictor)
#Automatically creates a boxplot if first predictor is quantitative
```

*Histogram*
```{r Histogram}
hist(dataset$predictor, breaks = 10)

#with conditions
hist(Dataset$predictor[Dataset$predictor>30])
```

*Plotting for outliers*
```{r plotting for outliers}
plot(predict(fit), rstudent(fit))
```

### Simulating data
```{r random data}
set.seed(1)
x=rnorm(100)
y=2*rnorm(100)

Data = data.frame(x, y)
```

### Linear Regression

```{r fitting linear models}
fit=lm(Y~x1+x2+x3+x4, data=dataset)

#no intercept
fit=lm(y~x+0)

#all predictors
fit=lm(Y~., dataset)

#all except
fit=lm(Y~.-x1-x2, dataset)

#Interactions
#between x1 & x2:
fit=lm(Y~x1*x2, dataset)

#all predictors + interaction
fit=lm(Y~.+x1:x2+x3:x4, dataset)

#QUAD/POLYNOMIAL
#Squared:
fit=lm(Y~x+I((x2)^2), dataset)
#Higher degree
fit=lm(Y~poly(x, 4))
```

*Extracting info from fits*
```{r fit info}
names(fit)
coef(fit)
summary(fit)$r.squared #R^2 of the model
summary(fit)$sigma #gives the RSE of the model
summary(fit)$coefficients[,"Pr(>|t|)"] #p-value for each coef
```

*Correlations*
```{r correlation matrix}
cor(dataset)
#NEED to exclude qualitative variable + AS FACTORS
cor(subset(dataset, select=-qualpred))
cor(Dataset[,-9]) #removing a column
```

*Function that fits & Plots at once*
```{r function to fit & plot}
regplot=function(y,x,...) #... means we have unnamed arguments but we are allowed to add more arguuments
{
  fit=lm(y~x)
  plot(x,y,...) #whatever extra parameters I add when calling the function will be added to plot()
  abline(fit, col="red")
}
regplot(Y, X, xlab="Response name", ylab="Predictor name", col="blue", pch=20)
```

*Dummy Variables*
```{r dummy variables - constrasts}
contrasts(dataset$quantpredictor)
```

*Comparing fits, ANOVA*
```{r comparing fits ANOVA}
anova(fit1, fit2)
 #large f-stat, small p-val, second model better
```

*Predictions, Confidence Intervals*
```{r predictions and confint}
#general confidence interval (95%)
confint(fit)

#predict for specific values of predictors
predict(fit, data.frame(predictor=(c(5,10,15))), interval="confidence")
predict(fit, data.frame(predictor=(c(5,10,15))), interval="prediction") 
```

*Diagnostic Plots for fit*
```{r fit diagnostic plot}
par(mfrow=c(2,2))
plot(fit)
```

### Variable Selection
```{r forward-backward-best}
#All Subset Regression
ols_step_all_possible(fit)

# Best Subset Regression
ols_step_best_subset(fit)

# Stepwise Forward Regression
ols_step_forward_p(fit)

# Stepwise Backward Regression
ols_step_backward_p(fit)
```

### Logistic Regression

*Training Set*
```{r training and test set}
#Splitting the Data Arbitrarily
train=(predictor<2005) #condition #boolean vector
test=Dataset[!train,] #subset of dataset that is "false" in train
dim(test)
Ytestval=Y[!train] #actual values for the response (category) in the test set
```

*Fitting the model*
```{r fitting model}
logist_reg_fit = glm(Y~X1+X2+X3..., data=dataset, family=binomial, subset=train)

#coefficients:
coef(logist_reg_fit)
summary(logist_reg_fit)$coef

#p-values:
summary(logist_reg_fit)$coef[ ,4]
```

*Predicting*
```{r predicting}
glm_probs = predict(logist_reg_fit, test, type="response") #vector of probabilities for each observation
#predicting over the test set using the training set model

contrasts(Y) #down vs up
#the category with value=1 is the one for which we are predicting the probability (alphabetical by default)

#no: contrast 0
#glm_pred=rep("no", (number_of_observations))
#glm_pred[glm_probs>.5]="yes" #yes constrast 1
#or
glm_pred=ifelse(glm_probs>0.5, "yes", "no") #no need to specify the number of observations

#Confusion Matrix:
table(glm_pred, Ytestval)

#Accuracy: (TP+TN)/Total
mean(glm_pred==Ytestval)
#Test set error: 
mean(glm_pred!=Ytestval)

#FNR=(FN/total positives)x100
#FPR=(FP/total negatives)x100 = 1-specificity = Type I error
#overall error=(FP+FN)/total x100
#sensitivity = TPR = (TP/Total positives) x100

#PREDICTING FOR GIVEN VALUES
predict(logist_reg_fit, newdata = data.frame(predictor1=c(1.2,1.5), predictor2=c(1.1,-0.8)), type="response")
```

*Function that combines all these steps*
```{r function that splits and gets the error}
#SET SEED FOR CONSISTENCY
#RANDOMIZED SPLIT
myf = function() {
  
    #set.seed()
    #splitting into training and validation set
    train = sample(dim(Dataset)[1], dim(Dataset)[1]/2)
    # fitting model using training
    glmfit = glm(Y ~ x1 + x2, data = Dataset, family = binomial, subset = train)
    # predict
    glmpred = rep("No", dim(Dataset)[1]/2) #"No" replace with contrasts=1
    glmprobs = predict(glm.fit, Default[-train, ], type = "response")
    glmpred[glm.probs > 0.5] = "Yes"
    # validation set error
    return(mean(glmpred != Dataset[-train, ]$Y))
}
myf()
```


### Linear Discriminant Analysis (LDA)

```{r fitting the model}
lda_fit = lda(Y~x1+x2, data=Dataset, subset=train)
lda_fit
plot(lda_fit) #produces plots of the linear discriminants
```

```{r predicting}
lda_pred = predict(lda_fit, test)
names(lda_pred) #the names are  class, contains LDA’s predictions about the movement of the market. posterior, matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class, x contains the linear discriminants

lda_class = lda_pred$class #the class predictions of our fitted model on test data
table(lda_class, Ytestval) #predictions vs real values of test data
contrasts(Y) #the one with value=1 is the one for which we are getting the posterior probability

#Accuracy
mean(lda_class==Ytestval) #this is the accuracy
#FNR=(FN/total positives)x100
#FPR=(FP/total negatives)x100 = 1-specificity = Type I error
#overall error=(FP+FN)/total x100
#sensitivity = TPR = (TP/Total positives) x100

sum(lda_pred$posterior[,1]>=0.5) #observations with predicted proba >0.5
sum(lda_pred$posterior[,1]<0.5)

#MY OWN POSTERIOR PROBABILITY THRESHOLD
sum(lda_pred$posterior[,1]>0.9) #observations with predicted proba >0.9
my_threshlold_classes=ifelse(lda_pred$posterior[,1]>0.9, "yes", "no") #yes being the one with contrast=1
```

### Quadratic Discriminant Analysis (QDA)

```{r fitting the model}
qda_fit=qda(Y~x1+x2, data=dataset, subset = train)
qda_fit

#CV????

qda_class=predict(qda_fit, test)$class
table(qda_class, Ytestval)

#Accuracy
mean(qda_class==Ytestval)
#FNR=(FN/total positives)x100
#FPR=(FP/total negatives)x100 = 1-specificity = Type I error
#overall error=(FP+FN)/total x100
#sensitivity = TPR = (TP/Total positives) x100
```

### KNN 

```{r fitting the model}
#1.trainX:  A matrix containing the predictors associated with the training data, labeled 
#2.testX: A matrix containing the predictors associated with the data for which we wish to make predictions
#3. trainDirection: A vector containing the class labels for the training observations
#4. A value for K, the number of nearest neighbors to be used by the classifier.

#test=1:1000
#train=Dataset[-test,]
library(class)
trainX=cbind(pred1, pred2)[train,]
testX=cbind(pred1, pred2)[!train,] #from the test data
#trainX=as.matrix((Lag2)[train]) #for single predictors
#testX=as.matrix((Lag2)[!train])
trainClass=Y[train]

set.seed(1)
knn_pred=knn(trainX, testX, trainClass, k=1)
table(knn_pred, Ytestval)

mean(knn_pred==Ytestval)
#FNR=(FN/total positives)x100
#FPR=(FP/total negatives)x100 = 1-specificity = Type I error
#overall error=(FP+FN)/total x100
#sensitivity = TPR = (TP/Total positives) x100
```

### Validation Set Approach
```{r random validation set}
set.seed(1)
train=sample(392, 196) #randomly pick 196 numbers from 1:392 which will be the indeces of the training sample
fit=lm(Y~x, data=Dataset, subset = train)
#test MSE Estimate
mean((Y-predict(fit, Dataset))[-train]^2)

#for quadratic
fit2=lm(Y~poly(x, 2), data=Dataset, subset=train)
#test MSE
mean((Y-predict(fit2, Dataset))[-train]^2)

#for cubic
fit3=lm(Y~poly(x, 3), data=Dataset, subset=train)
#test MSE
mean((Y-predict(x, Dataset))[-train]^2)

#If we choose a different training set instead, then we will obtain somewhat different errors on the validation set
```

### LOOCV
```{r LOOCV}
#glm() to fit a model without passing in the family argument, it performs linear regression, like lm()
#cv.glm() only works for glm()

glmfit=glm(Y~x, data=Dataset)
cv_err=cv.glm(Dataset, glmfit) #cv.glm default k=n
#cv.glm() function produces a list with several components
cv_err$delta #delta contain 2 results of cv, second is corrected for bias
```

*Computing CV Error for Multiple Degrees*
```{r computing cv error for multiple degrees}
#FUNCTION THAT COMPUTES CV ERROR FOR MULTIPLE DEGREES
cv_error_loocv=rep(0,n) #replicates the number 0,  n times
degree=1:n
for(d in degree)
{
  glmfit=glm(Y~poly(X, d), data=Dataset)
  cv_error_loocv[d]=cv.glm(Dataset, glmfit)$delta[1] #actual cross-validation error: delta[2] is the adjusted cv error
}
cv_error_loocv #vector with the cv errors of all the tested models
plot(degree, cv_error_loocv, type="b") #gives a plot of cv errors for each polynomial degree of our model
```

*Computing CV Error for additional degrees of same predictor*

```{r update poly }

#Y = β0 + β1X + ε
#Y = β0 + β1X + β2X^2 + ε
#Y = β0 +β1X +β2X^2 +β3X^3 +ε... so on

set.seed(1)
glm.fit = glm(y ~ x)
cv.glm(Data, glm.fit)$delta
glmfit=glm(Y~x, data=Dataset)
errorloocv=rep(0, n) #replicates the number 0,  n times
errorloocv[1]=cv.glm(Dataset, glmfit)$delta[1]
degree=2:n
for(d in degree)
{
  glmfit=update(glmfit, . ~ . + poly(x,d))
  errorloocv[d]=cv.glm(Dataset, glmfit)$delta[1] #actual cross-validation error: delta[2] is the adjusted cv error
}
errorloocv #vector with the cv errors of all the tested models
```


*Custom LOOCV Function*
```{r computing classification error}
count = rep(0, dim(Dataset)[1])
for (i in 1:(dim(Dataset)[1])) {
    glm.fit = glm(Y ~ x1 + x2, data = Dataset[-i, ], family = binomial)
    is_up = predict.glm(glm.fit, Dataset[i, ], type = "response") > 0.5
    #is_up should be replaced by is_contrasts=1
    is_true_up = Dataset[i, ]$Y == "Up"
    #whatever classification for contrasts=1
    if (is_up != is_true_up) 
        count[i] = 1
}
sum(count) #total number of errors
mean(count) #test error rate for LOOCV
```


### K-Fold Cross Validation
k-fold cross-validation is implemented by taking the set of n observations and randomly splitting into k non-overlapping groups. Each of these groups acts as a validation set and the remainder as a training set. The test error is estimated by averaging the k resulting MSE estimates.

```{r}
glmfit=glm(Y~x, data=Dataset)
cv_err=cv.glm(Dataset, glmfit, K=10) #change K

#FUNCTION THAT COMPUTES K-FOLD CV ERROR FOR MULTIPLE DEGREES
set.seed(1)
cv_error_k=rep(0,n)
degree=1:n
for (d in degree)
{
  glmfit=glm(Y~poly(x,d), data=Dataset)
  cv_error_k[d]=cv.glm(Dataset, glmfit, K=10)$delta[1]
  #change K according to preference
}
cv_error
plot(degree, cv_error_k, type="b")

#if we already plotted the same plot for loocv and wish to add the k fold line to it in red
#lines(degree, cv_error_k, type="b", col="red")
```

### Bootstrap
*Getting the Standard Error of an Estimate*
```{r bootstrap for estimates}
alphafn=function(data, index) #indeces
{
  X=data$X[index]
  Y=data$Y[index]
  return((var(Y)-cov(X,Y))/(var(X)+var(Y)-2*cov(X,Y))) #investment in slides #return any other function
}

#calculate estimate from first 100 elements of dataset:
alphafn(Dataset, 1:100) 

#one bootstrap iteration based on 100 indeces only
set.seed(1)
alphafn(Dataset, sample(100,100, replace = T)) 

#here we used the entire dataset 
bootout=boot(Dataset, alphafn, R=1000)
bootout #gives summary of bootstrap
#original+-stderror

plot(bootout)
```

*Estimating the Accuracy of a Linear Model*
```{r bootstrap for accuracy}
#assess the variability of the coefficient estimates and predictions from a statistical learning method

bootfn=function(data, index)
{return(coef(lm(Y~X, data=data, subset=index)))} #change Y and X, can also change the return value
bootfn(Dataset, 1:392) #returns the estimate for the subset of indices 1-->392

set.seed(1)
bootfn(Dataset, sample(392,392, replace = T)) #1 bootstrap iteration of a random bootstrap sample
boot(Dataset, bootfn, 1000) #1000 bootstrap iterations, returns the standard error of the coefficients in this case
#if the bootstrap std errors are relatively high, either high variability in the data or a certain grouping in the data, or the model is not the appropriate one

#bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of βˆ0 and βˆ1 than is the summary() function for a fit

##FOR A QUADRATIC MODEL
bootfn=function(data, index) #redefining it for a quadratic model
{
  coefficients(lm(Y~X+I(X^2), data=data, subset = index)) #change Y and X
}
set.seed(1)
boot(Dataset, bootfn, 1000)

#we should also calculate the mean squared error of the models to decide between linear and quadratic
#in this case we cannot decide based on the bootstrap alone
#using the coefficients from the bootstrap only might be problematic

#LOGISTIC REGRESSION
boot.fn = function(data, index) {return(coef(glm(Y ~ x1 + x2,
    data = Dataset, family = binomial, subset = index)))}
library(boot)
boot(Dataset, boot.fn, 50)
```

*SE Mean of a Predictor Using Bootstrap*
```{r standard error of mean}
#here data is the predictor itself (medv)
boot.fn = function(data, index) return(mean(data[index]))
library(boot)
bstrap = boot(predictor, boot.fn, 1000)
bstrap
```

*SE Median of a Predictor Using Bootstrap*
```{r}
library(boot)
#here data is the predictor itself (medv)
boot.fn = function(data, index) return(median(data[index]))
boot(predictor, boot.fn, 1000)
```

*Confidence Interval Using Bootstrap*
```{r confidence interval using bootstrap}
#here this is specific for the mean of a predictor
#95%
#c(bstrap$t0 - (2 * SE of mean), bstrap$t0 + (2 * SE of mean))
```

*Bootstrap for Percentile of a Predictor*
```{r bootstrap for 10th percentile of a predictor}
boot.fn = function(data, index) return(quantile(data[index], c(0.1)))
boot(predictor, boot.fn, 1000)
```



